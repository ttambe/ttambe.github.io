---
---

@inproceedings{10067817,
  author={Tambe, Thierry and Zhang, Jeff and Hooper, Coleman and Jia, Tianyu and Whatmough, Paul N. and Zuckerman, Joseph and Santos, Maico Cassel Dos and Loscalzo, Erik Jens and Giri, Davide and Shepard, Kenneth and Carloni, Luca and Rush, Alexander and Brooks, David and Wei, Gu-Yeon},
  booktitle={2023 IEEE International Solid- State Circuits Conference (ISSCC)}, 
  abbr = {ISSCC},
  title={A 12nm 18.1TFLOPs/W Sparse Transformer Processor with Entropy-Based Early Exit, Mixed-Precision Predication and Fine-Grained Power Management}, 
  abstract={Large language models have substantially advanced nuance and context understanding in natural language processing (NLP), further fueling the growth of intelligent conversational interfaces and virtual assistants. However, their hefty computational and memory demands make them potentially expensive to deploy on cloudless edge platforms with strict latency and energy requirements. To address this challenge, we present a 4.60mm2 sparse transformer processor (STP) that efficiently accelerates transformer workloads by tailoring its latency and energy expenditures according to the complexity of the input query it processes. Key contributions of this work are as follows: (1) A specialized datapath for entropy-based early exit assessment reduces BERT latency by up to 6.13x, e.g., inferences terminate early at an average early exit layer of 3.90 (out of 12) for the SST-2 NLP benchmark; (2) A mixed-precision (MP) FP4/FP8 MAC supports per-vector exponent biases during 4-bit floating point (FP4) computations, allowing the processor to double its throughput while reducing its energy consumption and maintaining high inference accuracy, depending on the entropy; and (3) A fine-grained sentence-level power management scheme opportunistically scales the accelerator’s supply voltage and clock frequency while meeting an application’s end-to-end latency target. Together, the proposed STP achieves a peak efficiency of 65mJ/inf, a 7.14x energy improvement, on average, over conventional BERT inference without the key innovations.},
  HTML = {https://ieeexplore.ieee.org/document/10067817},
  slides = {Tambe_ISSCC_2023_Presentation.pdf},
  year={2023}
}

@ARTICLE{9791855,
  author={Tambe, Thierry and Yang, En-Yu and Ko, Glenn G. and Chai, Yuji and Hooper, Coleman and Donato, Marco and Whatmough, Paul N. and Rush, Alexander M. and Brooks, David and Wei, Gu-Yeon},
  journal={IEEE Journal of Solid-State Circuits}, 
  abbr = {JSSC},
  title={A 16-nm SoC for Noise-Robust Speech and NLP Edge AI Inference With Bayesian Sound Source Separation and Attention-Based DNNs}, 
  year={2023},
  volume={58},
  number={2},
  code={https://github.com/harvard-acc/FlexASR},
  abstract={The proliferation of personal artificial intelligence (AI) -assistant technologies with speech-based conversational AI interfaces is driving the exponential growth in the consumer Internet of Things (IoT) market. As these technologies are being applied to keyword spotting (KWS), automatic speech recognition (ASR), natural language processing (NLP), and text-to-speech (TTS) applications, it is of paramount importance that they provide uncompromising performance for context learning in long sequences, which is a key benefit of the attention mechanism, and that they work seamlessly in polyphonic environments. In this work, we present a 25-mm2 system-on-chip (SoC) in 16-nm FinFET technology, codenamed SM6, which executes end-to-end speech-enhancing attention-based ASR and NLP workloads. The SoC includes: 1) FlexASR, a highly reconfigurable NLP inference processor optimized for whole-model acceleration of bidirectional attention-based sequence-to-sequence (seq2seq) deep neural networks (DNNs); 2) a Markov random field source separation engine (MSSE), a probabilistic graphical model accelerator for unsupervised inference via Gibbs sampling, used for sound source separation; 3) a dual-core Arm Cortex A53 CPU cluster, which provides on-demand single Instruction/multiple data (SIMD) fast fourier transform (FFT) processing and performs various application logic (e.g., expectation–maximization (EM) algorithm and 8-bit floating-point (FP8) quantization); and 4) an always-ON M0 subsystem for audio detection and power management. Measurement results demonstrate the efficiency ranges of 2.6–7.8 TFLOPs/W and 4.33–17.6 Gsamples/s/W for FlexASR and MSSE, respectively; MSSE denoising performance allowing 6x smaller ASR model to be stored on-chip with negligible accuracy loss; and 2.24-mJ energy consumption while achieving real-time throughput, end-to-end, and per-frame ASR latencies of 18 ms.},
  HTML = {https://ieeexplore.ieee.org/document/9791855},
  pages={569-581},
}

@report{zhang2023camel,
  title={CAMEL: Co-Designing AI Models and Embedded DRAMs for Efficient On-Device Learning},
  abbr={Arxiv},
  author={Zhang*, Sai Qian and Tambe*, Thierry and Cuevas, Nestor and Wei, Gu-Yeon and Brooks, David},
  arxiv={2305.03148},
  year={2023}
}

@inproceedings{10.1145/3466752.3480095,
author = {Tambe, Thierry and Hooper, Coleman and Pentecost, Lillian and Jia, Tianyu and Yang, En-Yu and Donato, Marco and Sanh, Victor and Whatmough, Paul and Rush, Alexander M. and Brooks, David and Wei, Gu-Yeon},
title = {EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480095},
abstract = {Transformer-based language models such as BERT provide significant accuracy improvement to a multitude of natural language processing (NLP) tasks. However, their hefty computational and memory demands make them challenging to deploy to resource-constrained edge platforms with strict latency requirements. We present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware energy optimizations for multi-task NLP. EdgeBERT employs entropy-based early exit predication in order to perform dynamic voltage-frequency scaling (DVFS), at a sentence granularity, for minimal energy consumption while adhering to a prescribed target latency. Computation and memory footprint overheads are further alleviated by employing a calibrated combination of adaptive attention span, selective network pruning, and floating-point quantization. Furthermore, in order to maximize the synergistic benefits of these algorithms in always-on and intermediate edge computing settings, we specialize a 12nm scalable hardware accelerator system, integrating a fast-switching low-dropout voltage regulator (LDO), an all-digital phase-locked loop (ADPLL), as well as, high-density embedded non-volatile memories (eNVMs) wherein the sparse floating-point bit encodings of the shared multi-task parameters are carefully stored. Altogether, latency-aware multi-task NLP inference acceleration on the EdgeBERT hardware system generates up to 7x, 2.5x, and 53x lower energy compared to the conventional inference without early stopping, the latency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson Tegra X2 mobile GPU, respectively.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {830–844},
numpages = {15},
keywords = {embedded non-volatile memories, natural language processing, latency-aware, software and hardware co-design},
location = {Virtual Event, Greece},
series = {MICRO '21},
abbr={MICRO},
HTML={https://dl.acm.org/doi/abs/10.1145/3466752.3480095},
slides={Tambe_MICRO_2021_Presentation.pdf},
code={https://github.com/harvard-acc/EdgeBERT},
pdf={edgebert_micro21_paper.pdf}
}


@inproceedings{9218516,
  author={Tambe, Thierry and Yang, En-Yu and Wan, Zishen and Deng, Yuntian and Janapa Reddi, Vijay and Rush, Alexander and Brooks, David and Wei, Gu-Yeon},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)}, 
  title={Algorithm-Hardware Co-Design of Adaptive Floating-Point Encodings for Resilient Deep Learning Inference}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abbr={DAC},
  award={Best Paper Award},
  abstract = {Conventional hardware-friendly quantization methods, such as fixed-point or integer, tend to perform poorly at very low precision as their shrunken dynamic ranges cannot adequately capture the wide data distributions commonly seen in sequence transduction models. We present an algorithm-hardware co-design centered around a novel floating-point inspired number format, AdaptivFloat, that dynamically maximizes and optimally clips its available dynamic range, at a layer granularity, in order to create faithful encodings of neural network parameters. AdaptivFloat consistently produces higher inference accuracies compared to block floating-point, uniform, IEEE-like float or posit encodings at low bit precision (≤ 8-bit) across a diverse set of state-of-the-art neural networks, exhibiting narrow to wide weight distribution. Notably, at 4-bit weight precision, only a 2.1 degradation in BLEU score is observed on the AdaptivFloat-quantized Transformer network compared to total accuracy loss when encoded in the above-mentioned prominent datatypes. Furthermore, experimental results on a deep neural network (DNN) processing element (PE), exploiting AdaptivFloat logic in its computational datapath, demonstrate per-operation energy and area that is 0.9X and 1.14X, respectively, that of an equivalent bit width NVDLA-like integer-based PE.},
  HTML={https://ieeexplore.ieee.org/document/9218516},
  pdf={Tambe_dac_2020_paper.pdf},
  code={https://github.com/ttambe/AdaptivFloat},
  slides={Tambe_DAC_2020_Presentation.pdf}
  }
